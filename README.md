# QuantMARLGA

**Multi-Agent Reinforcement Learning with Genetic Algorithm for Quantitative Trading**

RACE λ…Όλ¬Έ κΈ°λ° GA+MARL ν•μ΄λΈλ¦¬λ“ ν•κµ­ μ£Όμ‹ νΈλ μ΄λ”© μ‹μ¤ν…

---

## π“‹ ν”„λ΅μ νΈ κ°μ”

### ν•µμ‹¬ μ•„μ΄λ””μ–΄
- **4κ° μ „λ¬Έν™” μ—μ΄μ „νΈ**: Value, Quality, Portfolio, Hedging
- **λ³‘λ ¬ + μµν•© κµ¬μ΅°**: Value/Quality λ³‘λ ¬ β†’ Portfolio β†’ Hedging μμ°¨
- **RACE λ°©μ‹ ν•™μµ**: EA Population (GA μ§„ν™”) + MARL ν€ (RL ν•™μµ)
- **μ‹¤μ „ λ°μ΄ν„°**: ν•κµ­ μ£Όμ‹ μ‹μ¥ μ‹¤μ  λ°μ΄ν„° (KOSPI μƒμ„ μΆ…λ©)

### μ£Όμ” νΉμ§•
β… RACE λ…Όλ¬Έ λ°©μ‹ μ™„μ „ κµ¬ν„ (EA vs MARL λ¶„λ¦¬)  
β… Shared Replay Buffer (λ¨λ“  ν€ κ²½ν— κ³µμ )  
β… Dynamic Injection (MARL β†’ EA worst κµμ²΄)  
β… μ‹¤μ  μ‹μ¥ λ°μ΄ν„° νμ΄ν”„λΌμΈ  
β… μ°¨λ³„ν™” λ³΄μƒ ν•¨μ (μ—μ΄μ „νΈλ³„ κΈ°μ—¬λ„)  
β… 7μΆ… μ„±κ³Ό μ§€ν‘ (Sharpe, MDD, Calmar λ“±)  

---

## π—οΈ μ‹μ¤ν… κµ¬μ΅°

```
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚        EA Population (nκ°)              β”‚
β”‚  β”β”€β”€β”€β”€β”€β”€β” β”β”€β”€β”€β”€β”€β”€β”      β”β”€β”€β”€β”€β”€β”€β”       β”‚
β”‚  β”‚ EA 0 β”‚ β”‚ EA 1 β”‚ ...  β”‚ EA n β”‚       β”‚
β”‚  β””β”€β”€β”€β”€β”€β”€β” β””β”€β”€β”€β”€β”€β”€β”      β””β”€β”€β”€β”€β”€β”€β”       β”‚
β”‚     (GA μ§„ν™”λ§)                          β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
            β”‚
            β”β”€β”€β”€ Rollout β†’ Shared Buffer
            β”‚
β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
β”‚         MARL ν€ (1κ°)                   β”‚
β”‚  β”β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β”‚
β”‚  β”‚ Value β†’ Quality β†’ Portfolio β†’    β”‚  β”‚
β”‚  β”‚                    Hedging       β”‚  β”‚
β”‚  β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”  β”‚
β”‚     (RL ν•™μµλ§)                         β”‚
β””β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”€β”
            β”‚
            β”β”€β”€β”€ Rollout β†’ Shared Buffer
            β”‚
            β””β”€β”€β”€ RL Update β† Shared Buffer
            β”‚
            β””β”€β”€β”€ Injection β†’ EA worst
```

---

## π“ ν•™μµ λ£¨ν”„

### Phase 1: Pure GA (μ„Έλ€ 1-30)
1. Fitness ν‰κ°€ (λ°±ν…μ¤νΈ β†’ Sharpe Ratio)
2. Selection (Tournament + Elitism)
3. Crossover (Agent-level)
4. Mutation (Gaussian Noise)

### Phase 2: RACE Hybrid (μ„Έλ€ 31-100)
1. **Fitness ν‰κ°€** (EA Population)
2. **GA μ§„ν™”** (EA Populationλ§)
3. **Rollout** (EA nκ° + MARL 1κ° β†’ Shared Buffer)
4. **RL ν•™μµ** (MARL ν€λ§, λ¨λ“  κ²½ν— ν™μ©)
5. **Injection** (MARL β†’ EA worst κµμ²΄)
6. **λ‹¤μ μ„Έλ€ μ¤€λΉ„** (EA best β†’ MARL λ³µμ )

---

## π€ λΉ λ¥Έ μ‹μ‘

### ν™κ²½ μ„¤μ •
```bash
# Conda ν™κ²½ μƒμ„±
conda create -n quantagents python=3.10
conda activate quantagents

# ν¨ν‚¤μ§€ μ„¤μΉ
pip install torch numpy pandas scipy
pip install FinanceDataReader pykrx OpenDartReader
pip install python-dotenv tqdm
```

### OpenDart API ν‚¤ μ„¤μ •
```bash
# trading_marl_ga/.env νμΌ μƒμ„±
OPENDART_API_KEY=your_api_key_here
```

### λ°μ΄ν„° νμ΄ν”„λΌμΈ ν…μ¤νΈ
```bash
cd trading_marl_ga
python test_data_pipeline.py
```

### λ°±ν…μ¤νΈ ν™κ²½ ν…μ¤νΈ
```bash
python test_backtest_env.py
```

### GA Trainer ν…μ¤νΈ
```bash
python test_ga_trainer.py        # Pure GA
python test_race_hybrid.py       # RACE Hybrid
```

### λ²¤μΉλ§ν¬ λΉ„κµ
```bash
python benchmarks.py              # Buy & Hold vs Equal Weight vs Random
python final_comparison.py        # GA-MARL vs Benchmarks
```

---

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
QuantMARLGA/
β”β”€β”€ trading_marl_ga/
β”‚   β”β”€β”€ agents/                    # μ—μ΄μ „νΈ
β”‚   β”‚   β”β”€β”€ base_agent.py         # BaseAgent (Actor-Critic + GA)
β”‚   β”‚   β”β”€β”€ networks.py           # μ‹ κ²½λ§ (Actor, Critic)
β”‚   β”‚   β””β”€β”€ multi_agent_system.py # 4-Agent μ‹μ¤ν…
β”‚   β”‚
β”‚   β”β”€β”€ data/                      # λ°μ΄ν„° νμ΄ν”„λΌμΈ
β”‚   β”‚   β”β”€β”€ collectors/           # λ°μ΄ν„° μμ§‘κΈ°
β”‚   β”‚   β”‚   β”β”€β”€ price_collector.py
β”‚   β”‚   β”‚   β”β”€β”€ fundamental_collector.py
β”‚   β”‚   β”‚   β”β”€β”€ opendart_collector.py
β”‚   β”‚   β”‚   β””β”€β”€ financial_estimator.py
β”‚   β”‚   β””β”€β”€ market_data_manager.py # ν†µν•© κ΄€λ¦¬μ
β”‚   β”‚
β”‚   β”β”€β”€ environment/               # λ°±ν…μ¤νΈ ν™κ²½
β”‚   β”‚   β”β”€β”€ backtest_env.py       # λ§¤λ§¤ μ‹λ®¬λ μ΄μ…
β”‚   β”‚   β””β”€β”€ reward_calculator_independent.py
β”‚   β”‚
β”‚   β”β”€β”€ evolution/                 # GA + RACE
β”‚   β”‚   β””β”€β”€ ga_trainer.py         # RACE λ°©μ‹ GA Trainer
β”‚   β”‚
β”‚   β”β”€β”€ utils/                     # μ ν‹Έλ¦¬ν‹°
β”‚   β”‚   β”β”€β”€ observation.py        # κ΄€μΈ΅ κµ¬μ„±
β”‚   β”‚   β”β”€β”€ replay_buffer.py      # Shared Replay Buffer
β”‚   β”‚   β””β”€β”€ metrics.py            # μ„±κ³Ό μ§€ν‘
β”‚   β”‚
β”‚   β”β”€β”€ benchmarks.py              # λ²¤μΉλ§ν¬ μ „λµ
β”‚   β”β”€β”€ config.py                  # μ„¤μ •
β”‚   β””β”€β”€ test_*.py                  # ν…μ¤νΈ νμΌλ“¤
β”‚
β”β”€β”€ 1.md - 5.md                    # ν”„λ΅μ νΈ κ³„ν λ¬Έμ„
β”β”€β”€ IMPLEMENTATION_STATUS.md       # κµ¬ν„ ν„ν™©
β””β”€β”€ README.md                      # μ΄ νμΌ
```

---

## π“ μ„±κ³Ό μ§€ν‘

### κµ¬ν„λ μ§€ν‘ (7μΆ…)
- **Total Return**: μ΄ μμµλ¥ 
- **Sharpe Ratio**: μ„ν— λ€λΉ„ μμµλ¥ 
- **Max Drawdown**: μµλ€ λ‚™ν­
- **Win Rate**: μΉλ¥ 
- **Annualized Volatility**: μ—°μ¨ν™” λ³€λ™μ„±
- **Calmar Ratio**: μμµλ¥  / MDD
- **Sortino Ratio**: ν•λ°© μ„ν— μ΅°μ • μμµλ¥ 

### λ²¤μΉλ§ν¬ κ²°κ³Ό (100μΌ, 2023λ…„)
| μ „λµ | μμµλ¥  | μƒ¤ν”„ | MDD |
|------|--------|------|-----|
| Buy & Hold | 21.38% | 3.333 | -3.72% |
| KOSPI Index | 21.38% | 3.333 | -3.72% |
| Random Agent | 5.44% | 0.958 | -5.92% |

---

## π”¬ μ‹¤ν— μ„¤μ •

### λ°μ΄ν„°
- **μΆ…λ©**: KOSPI μ‹κ°€μ΄μ•΅ μƒμ„ 30κ° (10 β†’ 30 ν™•λ€)
- **κΈ°κ°„**: 2023λ…„ (μµμ† 200 κ±°λμΌ λ³΄μ¥)
- **Lookback**: 60 κ±°λμΌ (κΈ°μ μ  μ§€ν‘ κ³„μ‚°μ©)
- **λ¦¬λ°Έλ°μ‹±**: μ£Όκ°„ (5κ±°λμΌλ§λ‹¤)

### ν•μ΄νΌνλΌλ―Έν„° (μµμ ν™”λ¨ - 2025.11.24)
```python
# Environment
N_STOCKS = 30  # λ‹¤μ–‘μ„± μ¦κ°€
REBALANCE_PERIOD = 5  # κ±°λ λΉ„μ© μ κ°

# GA
POPULATION_SIZE = 10  # 30 β†’ 10 (ν¨μ¨μ„±)
N_GENERATIONS = 100
MUTATION_PROB = 0.9
MUTATION_SCALE_RATIO = 0.05  # μƒλ€μ  λ…Έμ΄μ¦
ELITE_FRACTION = 0.3  # μ•μ •μ„±

# RL
BATCH_SIZE = 256
BUFFER_CAPACITY = 10_000
MIN_BUFFER_FOR_RL = 256  # μ¦‰μ‹ ν•™μµ
LEARNING_RATE_ACTOR = 3e-4
LEARNING_RATE_CRITIC = 1e-3
GAMMA = 0.99

# Hybrid
RL_UPDATES = 50  # μ„Έλ€λ‹Ή

# GPU (μλ™ κ°μ§€)
DEVICE = "cuda" if available else "cpu"
USE_AMP = True  # FP16 (Colab)
```

---

## π“ λ¬Έμ„

- **[IMPLEMENTATION_STATUS.md](IMPLEMENTATION_STATUS.md)**: κµ¬ν„ ν„ν™© λ° μ§„ν–‰ μƒν™©
- **[2.md](2.md)**: 8μ‹κ°„ μ μ§„μ  κµ¬ν„ κ³„ν
- **[3.md](3.md)**: μ‹μ¤ν… μ „μ²΄ κµ¬μ΅° μ‹κ°ν™”
- **[4.md](4.md)**: λ³΄μƒ ν•¨μ μ„¤κ³„
- **[5.md](5.md)**: λ°μ΄ν„° νμ΄ν”„λΌμΈ μ„¤κ³„

---

## π― ν–¥ν›„ κ³„ν

### λ‹¨κΈ° (1-2μ£Ό)
- [ ] μ‹¤μ „ κ·λ¨ ν•™μµ (Population 30, 100μ„Έλ€)
- [ ] ν•™μµ κ³΅μ„  μ‹κ°ν™”
- [ ] μµμΆ… μ„±κ³Ό λ¶„μ„ λ¦¬ν¬νΈ

### μ¤‘κΈ° (1κ°μ›”)
- [ ] μΆ…λ© ν™•μ¥ (10 β†’ 50κ°)
- [ ] Walk-Forward Validation
- [ ] ν•μ΄νΌνλΌλ―Έν„° μλ™ νλ‹

### μ¥κΈ° (3κ°μ›”)
- [ ] Pre-training (Rule-Based Expert)
- [ ] μ‹¤μ‹κ°„ λ°μ΄ν„° μ—°λ™
- [ ] μλ™ λ§¤λ§¤ μΈν„°νμ΄μ¤

---

## π“ μ°Έκ³  λ¬Έν—

- **RACE λ…Όλ¬Έ**: Cooperative Multi-Agent Reinforcement Learning with Genetic Algorithm
- **FinanceDataReader**: https://github.com/FinanceData/FinanceDataReader
- **pykrx**: https://github.com/sharebook-kr/pykrx
- **OpenDartReader**: https://github.com/FinanceData/OpenDartReader

---

## π“„ λΌμ΄μ„ μ¤

MIT License

---

## π‘¥ κΈ°μ—¬

μ΄μ λ° PR ν™μν•©λ‹λ‹¤!

---

**μƒμ„±μΌ**: 2025-11-23  
**μµμΆ… μ—…λ°μ΄νΈ**: 2025-11-24 (GPU μµμ ν™”, ν•μ΄νΌνλΌλ―Έν„° νλ‹)  
**μ‘μ„±μ**: AI Assistant